# ğŸ›¡ï¸ QA Sentinel â€” Multi-Agent QA Pipeline

**Google ADK + Gemini + MCP**

A fully automated multi-agent QA system that transforms a user story into:

âœ… Structured features  
âœ… Complete test scenarios  
âœ… High-quality manual test cases  
âœ… Validation feedback  

â€” all orchestrated end-to-end using Google Agent Development Kit (ADK), Gemini models, LoopAgent validation, and MCP export tools.

**Built for Kaggle Agents Intensive (Capstone Project)**

Architected for clarity, robustness, reproducibility, and real-world QA workflows.

---

## ğŸš€ Overview

QA Sentinel is a production-grade agentic system designed to automate the entire QA planning workflow used in software development:

- **Planner Agent**: Breaks the story into features, scenarios, and acceptance-criteria mappings.
- **Test Case Generator Agent**: Generates Given/When/Then style manual test cases, edge cases, and bug risks.
- **Global Validator Agent**: Evaluates overall quality, alignment, structure, and consistency.
- **Orchestrator**: Manages pipeline execution, handles state, validates all ADK loop events, and ensures fully deterministic output.
- **Memory Layer (FAISS + JSON store)**: Retrieves similar past stories to improve future test-case generation.
- **Export Layer (MCP Server)**: Saves Markdown and JSON files from inside the ADK runtime to `/exports/...`.

This system mirrors how a real QA team works â€” but fully automated.

---

## ğŸ§© System Architecture

### High-Level System Architecture

```mermaid
graph TB
    subgraph "Input"
        A[User Story<br/>Title, Description, AC, QA Context]
    end
    
    subgraph "Orchestrator Layer"
        B[QASentinelOrchestrator<br/>Session Management & Coordination]
    end
    
    subgraph "Agent Layer"
        C[Story Planner Loop<br/>ADK v1 Loop]
        D[Test Case Generator Loop<br/>ADK v1 Loop]
        E[Global Validator Loop<br/>ADK v1 Loop]
    end
    
    subgraph "Memory Layer"
        F[QAStyleMemory<br/>FAISS Vector DB]
        G[SessionStore<br/>In-Memory State]
    end
    
    subgraph "Evaluation Layer"
        H[ConsistencyEvaluator<br/>Rule-based]
        I[A2AEvaluator<br/>Meta-evaluation]
    end
    
    subgraph "Export Layer"
        J[MCP Export Server<br/>Markdown & JSON]
    end
    
    subgraph "Output"
        K[Structured JSON<br/>Test Cases, Edge Cases,<br/>Bug Risks, Validation]
    end
    
    


At a high level, the system consists of:

- **ADK Loop Agents** - Three specialized agents using Google ADK v1 Loop pattern
- **Gemini models** - For all LLM tasks (Gemini 2.0 Flash)
- **Custom Orchestrator** - Manages pipeline execution and state
- **Consistency & A2A Evaluators** - Deterministic quality assessment
- **MCP File Export Server** - Model Context Protocol integration
- **Vector-based Memory Layer** - FAISS for pattern learning

Everything is modular, reusable, and extendable.

---

## ğŸ› ï¸ Components

### 1. Story Planner (LoopAgent)

Breaks the story into:
- **Features**: 3-8 high-level feature categories
- **Structured scenarios**: Auto-incremented (SC-1, SC-2...)
- **Acceptance criteria mapping**: Ensures every AC is mapped to scenarios
- **Notes/insights**: Domain considerations for QA

**Validation ensures:**
- âœ… Every AC is mapped
- âœ… Features list is non-empty
- âœ… JSON structure is strict

**Implementation**: ADK v1 Loop pattern using `ctx.llm.complete()` with Gemini 2.0 Flash. Includes automatic JSON parsing with markdown code block stripping.

### 2. Test Case Generator (LoopAgent)

Generates for each scenario:
- **1-3 high-quality test cases** with:
  - Preconditions
  - Given/When/Then steps
  - Expected result
- **Edge cases**: Boundary condition scenarios (EC-1, EC-2...)
- **Bug risks**: Potential failure modes and security concerns (BR-1, BR-2...)

**Validation ensures:**
- âœ… Tests reference scenarios
- âœ… Each test case includes Given + When + Then
- âœ… Expected result exists
- âœ… All scenarios are covered

**Implementation**: Hybrid QA format with Gherkin-style steps. Uses memory-retrieved examples to maintain style consistency.

### 3. Global Validator Agent

Checks:
- âœ… Cross-agent consistency
- âœ… Missing scenarios
- âœ… Missing test cases
- âœ… Logical alignment
- âœ… JSON shape correctness

**Validation Rules:**
1. Coverage Completeness: Every scenario maps to â‰¥1 test case
2. Step Quality: All test cases have clear Given/When/Then steps
3. Expected Result Quality: Results are specific and testable
4. Duplicate Detection: No redundant test cases
5. Edge Case Alignment: Edge cases are meaningful and related
6. Consistency: Titles, IDs, and flows match across agents
7. QA Context Alignment: Test cases reflect QA preferences

### 4. Orchestrator

The orchestrator:
- âœ… Sends ADK messages to each agent
- âœ… Handles retries, validation, and state delta
- âœ… Extracts JSON safely from LoopAgent events
- âœ… Generates outputs with timestamps
- âœ… Logs everything using the observability layer

It guarantees the pipeline never produces broken output.

---

## ğŸ§  Memory Layer (FAISS)

The Memory Layer stores:
- Story title
- Acceptance criteria
- Planner output
- Testcase output

**Uses:**
- FAISS vector search
- Title embeddings
- Top-K retrieval for similarity

Used by TestCaseGenerator to write smarter, more consistent test cases.

---

## ğŸ“ Evaluation Layer (Deterministic)

Two evaluators:

### âœ”ï¸ ConsistencyEvaluator

**Scores:**
- Scenario coverage
- GWT (Given/When/Then) structure
- Scenario referencing
- Plannerâ€“testcase structural validity

### âœ”ï¸ A2AEvaluator (Agent-to-Agent Meta Evaluation)

Mimics "agent reviewing another agent" using deterministic rules.

**Produces:**
- Component scores
- Qualitative reasoning
- Recommendations
- Coverage metrics

Used for Kaggle scoring alignment.

---

## ğŸ“¦ MCP Export Tool

MCP server provides tools:
- `save_markdown(filename, content)`
- `save_json(filename, data)`

**Agents can export:**
- Final test case bundles
- Planner outputs
- Validation reports
- Full pipeline dumps

**Saved under:**
```
exports/
  â””â”€â”€ markdown/
  â””â”€â”€ json/
```

---

## ğŸ“‚ Project Structure

```
qa-sentinel/
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ story_planner_agent.py
â”‚   â”œâ”€â”€ testcase_generator_agent.py
â”‚   â”œâ”€â”€ global_validator_agent.py
â”‚   â”œâ”€â”€ orchestrator_agent.py
â”‚   â””â”€â”€ adk_v1_compat.py
â”‚
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ qa_style_memory.py
â”‚   â””â”€â”€ session_store.py
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ consistency_evaluator.py
â”‚   â””â”€â”€ a2a_evaluator.py
â”‚
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ file_export_mcp.py
â”‚
â”œâ”€â”€ observability/
â”‚   â”œâ”€â”€ logging_config.py
â”‚   â””â”€â”€ tracing.py
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ model_config.py
â”‚   â””â”€â”€ settings.py
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ sample_input_story.md
â”‚   â””â”€â”€ sample_output_tests.md
â”‚
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ cloudrun_deploy.md
â”‚   â””â”€â”€ agent_engine_setup.md
â”‚
â”œâ”€â”€ exports/
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## ğŸ§ª How to Run Locally

### 1. Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Add Your Gemini API Key

Create `.env`:

```env
GOOGLE_API_KEY=your_key_here
```

### 4. Run Pipeline

```bash
python main_interactive.py
```

**What happens next:**
- `It will ask for User story title, Description, Acceptance Criteria and QA context.`
- `Then the pipeline will run and you will get the complete output.`

**Outputs will be saved in:**
- `exports/markdown/`
- `exports/json/`

---

## ğŸ” Example Output (Summary)

- **Planner generates**: SC-1 ... SC-n scenarios
- **Generator produces**: TC-1 ... TC-n test cases
- **Validator returns**: `{"valid": true, "errors": []}`
- **Evaluators produce**: Scores 0â€“100
- **Markdown files**: Saved automatically

### Sample Planner Output

```json
{
  "features": ["Profile Update - Name", "Profile Update - Email"],
  "scenarios": [
    {
      "scenario_id": "SC-1",
      "title": "Update Name Successfully",
      "acceptance_criteria": "User can update their name",
      "tags": ["positive", "name"]
    }
  ],
  "notes": ["Consider localization for name field"],
  "acceptance_criteria_input": ["User can update their name", ...]
}
```

### Sample Test Case Output

```json
{
  "test_cases": [
    {
      "id": "TC-1",
      "title": "Verify Successful Name Update - SC-1",
      "preconditions": ["User is logged in"],
      "steps": [
        "Given the user is on the profile page",
        "When the user updates their first name to 'John'",
        "Then the profile page should display 'John' as the updated name"
      ],
      "expected_result": "User's name is successfully updated and displayed."
    }
  ],
  "edge_cases": [{"id": "EC-1", "description": "Test with extremely long names"}],
  "bug_risks": [{"id": "BR-1", "description": "XSS vulnerability in name fields"}]
}
```

---

## â­ Why This Matters (Value Statement)

**QA Sentinel reduces:**
- â±ï¸ 6â€“10 hours of QA planning per sprint
- ğŸ”„ Manual duplication across acceptance criteria
- âŒ Gap-failures between scenarios & test cases

**And increases:**
- âœ… Consistency
- âœ… Coverage
- âœ… Edge-case discovery
- âœ… QA productivity

It represents the future of agentic QA automation.

---

## ğŸ”— Project Links

**GitHub Repo**: https://github.com/MhussainD4772/Capstone-Project-Agentic-AI-

---

## ğŸ“„ License

MIT License â€” see LICENSE file.

---

<div align="center">

**Built with Google ADK v1, Gemini 2.0 Flash, and Python**

â­ **Star this repo if you find it useful!** â­

</div>
